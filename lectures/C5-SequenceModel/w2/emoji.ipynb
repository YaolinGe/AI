{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emoji \n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❤️\n",
      "🎾\n",
      "😃\n",
      "😞\n",
      "🍴\n"
     ]
    }
   ],
   "source": [
    "def read_csv(filename = 'data/emojify_data.csv'):\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji.append(row[1])\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(emoji, dtype=int)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "emoji_dictionary = {\"0\": \"\\u2764\\ufe0f\",\n",
    "                    \"1\": \"\\U0001F3BE\",\n",
    "                    \"2\": \"\\U0001F603\",\n",
    "                    \"3\": \"\\U0001F61E\",\n",
    "                    \"4\": \"\\U0001F374\"}\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    return emoji_dictionary[str(label)]\n",
    "\n",
    "for label, emoji in emoji_dictionary.items():\n",
    "    print(label_to_emoji(label))\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again 😞\n",
      "I am proud of your achievements 😃\n",
      "It is the worst day in my life 😞\n",
      "Miss you so much ❤️\n",
      "food is life 🍴\n",
      "I love you mum ❤️\n",
      "Stop saying bullshit 😞\n",
      "congratulations on your acceptance 😃\n",
      "The assignment is too long  😞\n",
      "I want to go play 🎾\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = read_csv('emojidata/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('emojidata/test_emoji.csv')\n",
    "\n",
    "maxLen = len(max(X_train, key=len).split())\n",
    "for idx in range(10):\n",
    "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 'I missed you' has label index 0 which is emoji ❤️\n",
      "Label index 0 in one-hot encoding format is [1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)\n",
    "idx = 50\n",
    "print(f\"Sentence '{X_train[50]}' has label index {Y_train[idx]}\", f\"which is emoji {label_to_emoji(Y_train[idx])}\")\n",
    "print(f\"Label index {Y_train[idx]} in one-hot encoding format is {Y_oh_train[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('datasets/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "idx = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_word[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    words = sentence.lower().split()\n",
    "    avg = np.zeros(word_to_vec_map[any_word].shape)\n",
    "    count = 0 \n",
    "    for w in words:\n",
    "        if w in list(word_to_vec_map.keys()): \n",
    "            avg += word_to_vec_map[w]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        avg = avg / count \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = \n",
      " [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \\n\", avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, W, b, word_to_vec_map):\n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m, 1))\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    n_h = word_to_vec_map[any_word].shape[0] \n",
    "    for j in range(m):\n",
    "        words = X[j].lower().split()\n",
    "        avg = np.zeros((n_h,))\n",
    "        count = 0\n",
    "        for w in words:\n",
    "            if w in word_to_vec_map:\n",
    "                avg += word_to_vec_map[w]\n",
    "                count += 1\n",
    "        if count > 0:\n",
    "            avg = avg / count\n",
    "        Z = np.dot(W, avg) + b\n",
    "        A = softmax(Z)\n",
    "        pred[j] = np.argmax(A)\n",
    "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "    return pred\n",
    "\n",
    "\n",
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 200):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
    "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a valid word contained in the word_to_vec_map \n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "        \n",
    "    # Initialize cost. It is needed during grading\n",
    "    cost = 0\n",
    "    \n",
    "    # Define number of training examples\n",
    "    m = Y.shape[0]                             # number of training examples\n",
    "    n_y = len(np.unique(Y))                    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0]   # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations): # Loop over the number of iterations\n",
    "        for i in range(m):          # Loop over the training examples\n",
    "            \n",
    "            ### START CODE HERE ### (≈ 4 lines of code)\n",
    "            # Average the word vectors of the words from the i'th training example\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "\n",
    "            # Forward propagate the avg through the softmax layer\n",
    "            z = np.add(np.dot(W,avg),b)\n",
    "            a = softmax(z)\n",
    "\n",
    "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost = -np.sum(np.dot(Y_oh[i], np.log(a)))\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t % 10 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.864651019160555\n",
      "Accuracy: 0.45454545454545453\n",
      "Epoch: 10 --- cost = 0.9024757179755856\n",
      "Accuracy: 0.7575757575757576\n",
      "Epoch: 20 --- cost = 0.484798356756129\n",
      "Accuracy: 0.7803030303030303\n",
      "Epoch: 30 --- cost = 0.30265440959149736\n",
      "Accuracy: 0.7954545454545454\n",
      "Epoch: 40 --- cost = 0.2122654765980176\n",
      "Accuracy: 0.8181818181818182\n",
      "Epoch: 50 --- cost = 0.1615210985609624\n",
      "Accuracy: 0.8560606060606061\n",
      "Epoch: 60 --- cost = 0.13017043694575414\n",
      "Accuracy: 0.8787878787878788\n",
      "Epoch: 70 --- cost = 0.10934577304058635\n",
      "Accuracy: 0.8787878787878788\n",
      "Epoch: 80 --- cost = 0.09472272530884174\n",
      "Accuracy: 0.9015151515151515\n",
      "Epoch: 90 --- cost = 0.08399792095486905\n",
      "Accuracy: 0.9242424242424242\n",
      "Epoch: 100 --- cost = 0.07585322823985069\n",
      "Accuracy: 0.9318181818181818\n",
      "Epoch: 110 --- cost = 0.06948873316766034\n",
      "Accuracy: 0.9318181818181818\n",
      "Epoch: 120 --- cost = 0.06439501477754485\n",
      "Accuracy: 0.9318181818181818\n",
      "Epoch: 130 --- cost = 0.060234475375187474\n",
      "Accuracy: 0.9318181818181818\n",
      "Epoch: 140 --- cost = 0.056775844471423434\n",
      "Accuracy: 0.9393939393939394\n",
      "Epoch: 150 --- cost = 0.05385619123658323\n",
      "Accuracy: 0.9393939393939394\n",
      "Epoch: 160 --- cost = 0.05135793797332921\n",
      "Accuracy: 0.9393939393939394\n",
      "Epoch: 170 --- cost = 0.049194441471227895\n",
      "Accuracy: 0.946969696969697\n",
      "Epoch: 180 --- cost = 0.047300666968721004\n",
      "Accuracy: 0.9545454545454546\n",
      "Epoch: 190 --- cost = 0.04562699391320107\n",
      "Accuracy: 0.9545454545454546\n",
      "[[3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.9621212121212122\n",
      "Test set:\n",
      "Accuracy: 0.6071428571428571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'❤️'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)\n",
    "\n",
    "def predict_single(sentence, W=W, b=b, word_to_vec_map=word_to_vec_map):\n",
    "\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0] \n",
    "\n",
    "    # Split jth test example (sentence) into list of lower case words\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    # Average words' vectors\n",
    "    avg = np.zeros((n_h,))\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w in word_to_vec_map:\n",
    "            avg += word_to_vec_map[w]\n",
    "            count += 1\n",
    "\n",
    "    if count > 0:\n",
    "        avg = avg / count\n",
    "\n",
    "    # Forward propagation\n",
    "    Z = np.dot(W, avg) + b\n",
    "    A = softmax(Z)\n",
    "    pred = np.argmax(A)\n",
    "\n",
    "    return pred\n",
    "\n",
    "label_to_emoji(predict_single(\"I love you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "\n",
      "i adore you ❤️\n",
      "i love you ❤️\n",
      "funny lol 😃\n",
      "lets play with a ball 🎾\n",
      "food is ready 🍴\n",
      "not feeling happy 😃\n",
      "I hate you 😞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nq9093\\AppData\\Local\\Temp\\ipykernel_7560\\2635300693.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\", \"I hate you\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "\n",
    "def print_predictions(X, pred):\n",
    "    print()\n",
    "    for i in range(X.shape[0]):\n",
    "        print(X[i], label_to_emoji(int(pred[i])))\n",
    "\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56,)\n",
      "           ❤️    🎾    😃    😞   🍴\n",
      "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
      "Actual                                 \n",
      "0            5    1    4    2    0   12\n",
      "1            0    5    0    0    0    5\n",
      "2            2    2   11    3    0   18\n",
      "3            1    1    3    9    1   15\n",
      "4            0    0    1    1    4    6\n",
      "All          8    9   19   15    5   56\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "    \n",
    "    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
    "    \n",
    "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)\n",
    "    \n",
    "\n",
    "print(Y_test.shape)\n",
    "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "plot_confusion_matrix(Y_test, pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "np.random.seed(0)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I\n",
      "1 like\n",
      "2 learning\n"
     ]
    }
   ],
   "source": [
    "for idx, val in enumerate([\"I\", \"like\", \"learning\"]):\n",
    "    print(idx, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]                                   \n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):                               \n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j = j + 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices =\n",
      " [[155345. 225122.      0.      0.      0.]\n",
      " [220930. 286375.  69714.      0.      0.]\n",
      " [151204. 192973. 302254. 151349. 394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1, word_to_index, max_len=5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1                  \n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][1] = 0.39031\n",
      "Input_dim 400001\n",
      "Output_dim 50\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][1] =\", embedding_layer.get_weights()[0][1][1])\n",
    "print(\"Input_dim\", embedding_layer.input_dim)\n",
    "print(\"Output_dim\",embedding_layer.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmojifyLSTM(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 10, 50)            20000050  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 128)           91648     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 645       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20223927 (77.15 MB)\n",
      "Trainable params: 223877 (874.52 KB)\n",
      "Non-trainable params: 20000050 (76.29 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = EmojifyLSTM((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 2s 12ms/step - loss: 1.5785 - accuracy: 0.2348\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4845 - accuracy: 0.3788\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4420 - accuracy: 0.3030\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.3647 - accuracy: 0.4318\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.2384 - accuracy: 0.5530\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.0922 - accuracy: 0.5606\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.9496 - accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8656 - accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8408 - accuracy: 0.6742\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6896 - accuracy: 0.7803\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5363 - accuracy: 0.8106\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5724 - accuracy: 0.7879\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6453 - accuracy: 0.7879\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5355 - accuracy: 0.7803\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5023 - accuracy: 0.8030\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4676 - accuracy: 0.8409\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4498 - accuracy: 0.8561\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3714 - accuracy: 0.8939\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3712 - accuracy: 0.8636\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2985 - accuracy: 0.8864\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2865 - accuracy: 0.9242\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3106 - accuracy: 0.9091\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.2758 - accuracy: 0.8939\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2836 - accuracy: 0.9091\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3488 - accuracy: 0.8712\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2965 - accuracy: 0.8788\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1819 - accuracy: 0.9545\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1662 - accuracy: 0.9470\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2182 - accuracy: 0.9167\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1598 - accuracy: 0.9621\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1300 - accuracy: 0.9621\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1456 - accuracy: 0.9697\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1291 - accuracy: 0.9470\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0648 - accuracy: 0.9924\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1169 - accuracy: 0.9545\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0587 - accuracy: 0.9848\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0647 - accuracy: 0.9848\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0482 - accuracy: 0.9848\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0729 - accuracy: 0.9697\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0399 - accuracy: 0.9773\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0718 - accuracy: 0.9773\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0276 - accuracy: 0.9924\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0186 - accuracy: 0.9924\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0268 - accuracy: 0.9924\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.2258e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9.0802e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9.8349e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x249821a7810>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)\n",
    "\n",
    "model.fit(X_train_indices, Y_train_oh, epochs = 100, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4266 - accuracy: 0.6071\n",
      "\n",
      "Test accuracy =  0.6071428656578064\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Expected emoji:😃 prediction: he got a raise\t😞\n",
      "Expected emoji:❤️ prediction: she got me a present\t😃\n",
      "Expected emoji:❤️ prediction: he is a good friend\t😃\n",
      "Expected emoji:❤️ prediction: I am upset\t😞\n",
      "Expected emoji:❤️ prediction: We had such a lovely dinner tonight\t😃\n",
      "Expected emoji:😞 prediction: This girl is messing with me\t❤️\n",
      "Expected emoji:😃 prediction: are you serious ha ha\t😞\n",
      "Expected emoji:😞 prediction: stop messing around\t🎾\n",
      "Expected emoji:❤️ prediction: I love taking breaks\t😞\n",
      "Expected emoji:😞 prediction: she is a bully\t❤️\n",
      "Expected emoji:😞 prediction: I worked during my birthday\t😃\n",
      "Expected emoji:😃 prediction: enjoy your break\t🎾\n",
      "Expected emoji:❤️ prediction: valentine day is near\t😃\n",
      "Expected emoji:🍴 prediction: I am starving\t😞\n",
      "Expected emoji:😃 prediction: I will go dance🎾\n",
      "Expected emoji:😃 prediction: I like your jacket \t❤️\n",
      "Expected emoji:🎾 prediction: what is your favorite baseball game\t❤️\n",
      "Expected emoji:😃 prediction: I want to joke\t😞\n",
      "Expected emoji:😞 prediction: go away\t🎾\n",
      "Expected emoji:😞 prediction: yesterday we lost again\t🎾\n",
      "Expected emoji:😃 prediction: You totally deserve this prize\t😞\n",
      "Expected emoji:😞 prediction: I did not have breakfast ❤️\n"
     ]
    }
   ],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "I am not feeling happy 😞\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array([\"I am not feeling happy\"])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
